I"Í<h3 id="background">Background</h3>
<p>I bought a book on data-driven dynamical system earlier this year right before the pandemic spread in the US (ref. 1). The book was writen by two professors at University of Washington and was published quite recently. It talks about dynamical systems, but instead of focusing on the classical theories that we learnt in school, it focuses more on how data can help us identify and reduce the order of the systems. Session 7.2 of the book talks about a method named the Dynamic Mode Decomposition (DMD). I find it interesting because essentially the method enables us to discover, characterize and predict how a dynamical system evolves using data. More importantly the characterization could be low order, which means if the data is from a high fidelity model, the method gives us a way to make a reduced order model. This post summarizes my current high-level understanding of DMD after reading Session 7.2.</p>

<!--more-->
<h3 id="what-dmd-does">What DMD Does</h3>
<p>My understanding of DMD is that it tries to learn a linear dynamical system from data. Unlike training a deep learning neural network, however, DMD aims to do the ‚Äúlearning‚Äù fast and efficiently. The data fed into DMD are pairs of the state of a system separated by a small time interval $\Delta t$. By digesting these pairs of ‚Äúbefore and after‚Äù data, DMD learns to predict how the system evolves in the time period of $\Delta t$. And if you know how the system changes in a small time scale, you can predict its state at any time after an initial state is given.</p>

<h3 id="how-dmd-works">How DMD Works</h3>
<p>Let‚Äôs first talk about the data fed into the DMD algorithm. Denote $\vec{x}=[x_1, x_2, \cdots, x_n]^T$ as the state of a $n$-degrees-of-freedom dynamical system. Note that it is a column vector. We select a few, say $m$, snapshots of the system at different time and arrange these column vectors into a matrix $\textbf{X} = [\vec{x}(t_1), \vec{x}(t_2), \cdots, \vec{x}(t_m)]$. At each snapshot, we also get the state of the system after a time delay $\Delta t$, and arrange these states in a second matrix: $\textbf{X}‚Äô = [\vec{x}(t_1+\Delta t), \vec{x}(t_2+\Delta t), \cdots, \vec{x}(t_m+\Delta t)]$. We could like to find a linear dynamical system such that:
\begin{equation}
\textbf{X}‚Äô = \textbf{A} \textbf{X}.
\end{equation}</p>

<p>Naturally, one would think that if we could do an ‚Äúinverse‚Äù of the $n \times m$ matrix $\textbf{X}$ so that $\textbf{A} = \textbf{X}‚Äô \textbf{X}^{-1}$, then mission is accomplished. This is indeed a valid thought and SVD (singular value decomposition) can help us perform the psudo inverse. The problem is that when we have a large $n$, computing the $n \times n$ matrix $\textbf{A}$ becomes expensive, not to mention that subsequently we may need to further compute some properties of the system by manipulating $\textbf{A}$. DMD gave us a procedure to find $\textbf{A}$ efficiently.
Since the main issue is that we may have large $n$, let‚Äôs imagine we have a magic solution to reduce the order of the system so that:
\begin{equation}
\vec{x} = \textbf{U} \vec{y}.
\end{equation}
Here $\textbf{U}$ is a $n \times r$ matrix, and $\vec{y}$, our magic new state with less degrees of freedom, is a $r \times 1$ vector with $r &lt; n$.</p>

<p>Assuming we know how to compute $\textbf{U}$ so that the reduced order $\vec{y}$ still captures most of the infomration stored in $\vec{x}$, then in the reduced $y$ space, the $r \times r$ linear transformation matrix $\textbf{A}_y$ should be less expensive to compute:
\begin{equation}
\textbf{Y}‚Äô = \textbf{A}_y \textbf{Y}.
\end{equation}
And later we can recover our real $\textbf{A}$ using $\textbf{U}^T \textbf{A}_y \textbf{U}$.</p>

<p>The final trick is decouple the linear system by another transform. This is a classical eigen problem exercise, we will find a tranform $\textbf{W}$:
\begin{equation}
\vec{y} = \textbf{W} \vec{z},
\end{equation}
so that 
\begin{equation}
\textbf{Z}‚Äô = \textbf{A}_z \textbf{Z}.
\end{equation}
Here $\textbf{A}_z$ is a $r \times r$ diagonal matrix. In the $z$ space, the decoupled variables evolve following:
\begin{equation}
z_i(t) = \exp \left(\lambda_i t \right) z_i(0).
\end{equation}
Here $i=1,2,\cdots,r$.</p>

<p>To summarize, we need to do two transforms, one from $\vec{x}$ to $\vec{y}$, followed by a second one from $\vec{y}$ to $\vec{z}$, to find the simple dynamics that matches the data. Note the ‚Äúmatching‚Äù is achieved when we compute $\textbf{A}_y$. Then we revert those transforms to predict how $X$ will evolve.</p>

<h3 id="the-unexplained-magic-step">The Unexplained Magic step</h3>
<p>I have left the ‚Äúmagic‚Äù step on transforming $\vec{x}$ to $\vec{y}$ unexplained above. This order reducing step can be achieved by using SVD decomposition of the $\textbf{X}$ matrix.</p>

<h3 id="reference">Reference</h3>
<p>1.Brunton, S. L. &amp; Kutz, J. N. Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control. (Cambridge University Press, 2019). doi:10.1017/9781108380690.</p>
:ET