I"¶Ÿ<p>Recently I was following an <a href="http://vision.stanford.edu/teaching/cs231n/syllabus.html">online course</a> on Convolutional Neural Networks (CNN) provided by Stanford. I find it a very nice hands-on material: slides and notes are easy to understand. Purely reading formulations can be confusing sometimes, but practicing experiments helps better understanding what the formulations and the symbols in them are expressing.</p>

<p>The first assignment is about basic assignments. It also includes some practice on vectorization, which may make Python code faster. I donâ€™t do the nested version in some assignments, and some of code are half-vectorized.</p>

<h2 id="prerequisites">Prerequisites</h2>
<ul>
  <li><a href="http://cs231n.github.io/python-numpy-tutorial/#numpy-broadcasting">Broadcast in numpy</a></li>
</ul>

<h2 id="1-knn">1. KNN</h2>
<p>KNN is the easiest one; this part is still worth doing, because it helps understand vectorization and cross validation.</p>

<h3 id="train">Train</h3>
<p>In KNN, the process of training is simply remembering <code class="language-plaintext highlighter-rouge">X_train</code> and <code class="language-plaintext highlighter-rouge">y_train</code>:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">X_train</code>: Shape as (#features, #train). Each column corresponds to a training sample.</li>
  <li><code class="language-plaintext highlighter-rouge">y_train</code>: Shape as (#train,). Labels.</li>
</ul>

<h3 id="distances">Distances</h3>
<p>Essence of KNN lies in computing distances. Input is simply <code class="language-plaintext highlighter-rouge">X</code>. Output is <code class="language-plaintext highlighter-rouge">dists</code>.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">X</code>: Shape as (#features, #test). Each column corresponds to a test sample.</li>
  <li><code class="language-plaintext highlighter-rouge">XT</code>: Shape as (#features, #train). Each column corresponds to a training sample.</li>
  <li><code class="language-plaintext highlighter-rouge">dists</code>: Shape as (#test, #train). <code class="language-plaintext highlighter-rouge">dists[i, j]</code> means the distance of $i^{th}$ test sample to $j^{th}$ training sample.</li>
</ul>

<p>For brevity, denote</p>

<ul>
  <li>$D$ as #features</li>
  <li>$X^{(i)}$ as $i^{th}$ column of $X$, i.e. $i^{th}$ sample</li>
</ul>

<p>Since
\(\eqalign {
dists_{i,j} &amp;= \|X^{(i)} - XT^{(j)}\| \\
&amp;= \sqrt{\sum_{k=0}^{D-1}(X^{(i)}_k - XT^{(j)}_{k})^2} \\
&amp;= \sqrt{\sum_{k=0}^{D-1}{(X^{(i)}_k)^2} + \sum_{k=0}^{D-1}{(XT^{(j)}_k)^2} - 2\sum_{k=0}^{D-1}{X^{(i)}_{k} \cdot XT^{(j)}_{k}}} \\
&amp;= \sqrt{\|X^{(i)}\|^2 + \|XT^{(j)}\|^2 - 2(X^{(i)})^{T} \cdot XT^{(j)} }
}\)</p>

<p>, we can vectorize it like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">compute_distances_no_loops</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="n">num_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">num_train</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">dists</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_test</span><span class="p">,</span> <span class="n">num_train</span><span class="p">))</span>
    
    <span class="c1"># vectorize
</span>    <span class="n">ip</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">X_train</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> <span class="c1"># inner product
</span>    <span class="n">XT2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">X_train</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">X</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">dists</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="o">*</span><span class="n">ip</span> <span class="o">+</span> <span class="n">XT2</span> <span class="o">+</span> <span class="n">X2</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="predict">Predict</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
  <span class="k">def</span> <span class="nf">predict_labels</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dists</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">num_test</span> <span class="o">=</span> <span class="n">dists</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_test</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">num_test</span><span class="p">):</span>
      <span class="c1"># pick nearest neighbors  
</span>      <span class="n">closest_y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">y_train</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">dists</span><span class="p">[</span><span class="n">i</span><span class="p">])[:</span><span class="n">k</span><span class="p">]]</span>
      
      <span class="c1"># count which class appears most
</span>      <span class="n">y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">closest_y</span><span class="p">).</span><span class="n">most_common</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="k">return</span> <span class="n">y_pred</span>
</code></pre></div></div>

<h3 id="cross-validation">Cross validation</h3>

<p>Cross validation is a process to determine hyper-parameters; in this case, <code class="language-plaintext highlighter-rouge">k</code> is a hyper-parameter. We have <code class="language-plaintext highlighter-rouge">X_train</code> and <code class="language-plaintext highlighter-rouge">X_test</code>; now we subdivide <code class="language-plaintext highlighter-rouge">X_train</code> into <code class="language-plaintext highlighter-rouge">cv_X_train</code> and <code class="language-plaintext highlighter-rouge">cv_X_test</code>, where <code class="language-plaintext highlighter-rouge">cv_</code> means cross validation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_folds</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">k_choices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>

<span class="n">X_train_folds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">num_folds</span><span class="p">))</span>
<span class="n">y_train_folds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array_split</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">num_folds</span><span class="p">))</span>

<span class="n">k_to_accuracies</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">dim</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="kn">from</span> <span class="nn">cs231n.classifiers</span> <span class="kn">import</span> <span class="n">KNearestNeighbor</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_choices</span><span class="p">:</span>
    <span class="n">k_to_accuracies</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_folds</span>
    <span class="k">for</span> <span class="n">test_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_folds</span><span class="p">):</span>
        <span class="n">train_ids</span> <span class="o">=</span> <span class="p">[</span><span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_folds</span><span class="p">)</span> <span class="k">if</span> <span class="n">idx</span> <span class="o">!=</span> <span class="n">test_idx</span><span class="p">]</span>
        <span class="n">cv_X_train</span> <span class="o">=</span> <span class="n">X_train_folds</span><span class="p">[</span><span class="n">train_ids</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="n">cv_X_test</span> <span class="o">=</span> <span class="n">X_train_folds</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
        <span class="n">cv_y_train</span> <span class="o">=</span> <span class="n">y_train_folds</span><span class="p">[</span><span class="n">train_ids</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">cv_y_test</span> <span class="o">=</span> <span class="n">y_train_folds</span><span class="p">[</span><span class="n">test_idx</span><span class="p">]</span>
        
        <span class="n">classifier</span> <span class="o">=</span> <span class="n">KNearestNeighbor</span><span class="p">()</span>
        <span class="n">classifier</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">cv_X_train</span><span class="p">,</span> <span class="n">cv_y_train</span><span class="p">)</span>
        <span class="n">dists</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">.</span><span class="n">compute_distances_no_loops</span><span class="p">(</span><span class="n">cv_X_test</span><span class="p">)</span>
        <span class="n">cv_y_test_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">.</span><span class="n">predict_labels</span><span class="p">(</span><span class="n">dists</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
        
        <span class="n">num_correct</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">cv_y_test_pred</span> <span class="o">==</span> <span class="n">cv_y_test</span><span class="p">)</span>
        <span class="n">num_test</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">cv_y_test</span><span class="p">)</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">num_correct</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_test</span>
        <span class="n">k_to_accuracies</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">test_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">accuracy</span>
        
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">k_to_accuracies</span><span class="p">):</span> <span class="c1"># Print out the computed accuracies
</span>    <span class="k">for</span> <span class="n">accuracy</span> <span class="ow">in</span> <span class="n">k_to_accuracies</span><span class="p">[</span><span class="n">k</span><span class="p">]:</span>
        <span class="k">print</span> <span class="s">'k = %d, accuracy = %f'</span> <span class="o">%</span> <span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">accuracy</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="2-svm">2. SVM</h2>

<p>Here I omit the pre-processing part â€“ subtract the mean image and add an extra dimension â€˜1â€™ to each sample, which has been explained by lecture notes.</p>

<p>Denote</p>

<ul>
  <li>$x^{i}$ as $i^{th}$ column of $X$, i.e. $i^{th}$ sample</li>
  <li>$w_{j}$ as $j^{th}$ row of $W$</li>
  <li>$N$ as # of training samples</li>
  <li>$C$ as # of classes</li>
</ul>

<p>Then</p>

<ul>
  <li>
    <p>Score function <code class="language-plaintext highlighter-rouge">scores</code> or <code class="language-plaintext highlighter-rouge">wx</code>: 
\(f(x^{(i)}, W) = W x^{(i)}\)</p>
  </li>
  <li>
    <p>Loss function: 
\(L_{i} = \sum_{j \ne y_{i}}{\max(0, f(x^{(i)}, W)_{j} - f(x^{(i)}, W) + \Delta)}\)</p>
  </li>
  <li>
    <p>Gradient of loss <code class="language-plaintext highlighter-rouge">ddW</code>: 
\(\nabla_{w_{j}} L_{i} = \cases {
{\bf 1}(w_{j}x^{(i)} - w_{y_{i}}x^{(i)} + \Delta &gt; 0) \cdot x^{(i)} &amp; \text{, if } j \ne y_{i} \\
-\left( \sum_{j \ne y_{i}}{\bf 1}(w_{j}x^{(i)} - w_{y_{i}}x^{(i)} + \Delta &gt; 0) \right) \cdot x^{(i)} &amp; \text{, otherwise}
}\)</p>
  </li>
  <li>
    <p>Total loss <code class="language-plaintext highlighter-rouge">loss</code>:
\(L = \frac{1}{N}\sum_{i}{L_{i}} + \frac{1}{2}\lambda\sum_{k,l}{w_{k,l}^{2}}\)</p>
  </li>
  <li>
    <p>Gradient of total loss <code class="language-plaintext highlighter-rouge">dW</code>:
\(\nabla_{W}L = \frac{1}{N}\sum_{i}{\nabla_{W} L_{i}} + \lambda W\)</p>
  </li>
</ul>

<p>_<code class="language-plaintext highlighter-rouge">variable</code>_s that represent these terms are used in the following snippet.</p>

<h3 id="train-1">Train</h3>

<p>Training is to obtain best $W$, minimizing total loss $L$. Here we use gradient descent to find best $W$, and we need to calculate total loss and gradient of it.</p>

<h4 id="loss-by-nested-loop">Loss by nested loop</h4>

<p>Outer loop iterates <code class="language-plaintext highlighter-rouge">i</code> of $L_{i}$ over $N$, corresponding to a specific sample. Inner loop iterates <code class="language-plaintext highlighter-rouge">j</code> of $W_{j}$ over $C$, corresponding to a specific class.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">svm_loss_naive</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span>
  <span class="n">dW</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">W</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># initialize the gradient as zero
</span>  <span class="n">delta</span> <span class="o">=</span> <span class="mi">1</span>

  <span class="n">num_classes</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="n">num_train</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.0</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">num_train</span><span class="p">):</span>
    <span class="n">ddW</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">W</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">ddWyi</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">W</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="n">i</span><span class="p">])</span>
    <span class="n">correct_class_score</span> <span class="o">=</span> <span class="n">scores</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">num_classes</span><span class="p">):</span>
      <span class="k">if</span> <span class="n">j</span> <span class="o">==</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
        <span class="k">continue</span>
      <span class="n">margin</span> <span class="o">=</span> <span class="n">scores</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="n">correct_class_score</span> <span class="o">+</span> <span class="n">delta</span>
      <span class="k">if</span> <span class="n">margin</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">+=</span> <span class="n">margin</span>
        <span class="n">ddW</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="c1">## be careful, it's a reference
</span>        <span class="n">ddWyi</span> <span class="o">+=</span> <span class="n">ddW</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
    <span class="n">ddW</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="o">-</span><span class="n">ddWyi</span>
    <span class="n">dW</span> <span class="o">+=</span> <span class="n">ddW</span>
  
  <span class="c1"># divided by num_train
</span>  <span class="n">loss</span> <span class="o">/=</span> <span class="n">num_train</span>
  <span class="n">dW</span> <span class="o">/=</span> <span class="n">num_train</span>
  
  <span class="c1"># add regularization term
</span>  <span class="n">loss</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">W</span> <span class="o">*</span> <span class="n">W</span><span class="p">)</span>
  <span class="n">dW</span> <span class="o">+=</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">W</span>
  
  <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">dW</span>
</code></pre></div></div>

<h4 id="loss-by-half-vectorization">Loss by half vectorization</h4>

<p>Inner loop can be vectorized. Rather than count <code class="language-plaintext highlighter-rouge">margin</code> loop by loop under <code class="language-plaintext highlighter-rouge">j</code>, we can count them in a once. It depends on <code class="language-plaintext highlighter-rouge">scores</code> and <code class="language-plaintext highlighter-rouge">correct_class_score</code>. <code class="language-plaintext highlighter-rouge">scores</code> already satisfies, so weâ€™ll try to obtain <code class="language-plaintext highlighter-rouge">correct_class_score</code> for each <code class="language-plaintext highlighter-rouge">j</code>.</p>

<p>Sorry about naming, but here I name <code class="language-plaintext highlighter-rouge">wx</code> as <code class="language-plaintext highlighter-rouge">scores</code>, <code class="language-plaintext highlighter-rouge">judge</code> as <code class="language-plaintext highlighter-rouge">margin</code>s. Hereâ€™s the snippet:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">svm_loss_vectorized</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span>
  <span class="n">dW</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">W</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># initialize the gradient as zero
</span>  <span class="n">wx</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
  <span class="n">delta</span> <span class="o">=</span> <span class="mi">1</span>

  <span class="c1">### loss
</span>  <span class="c1"># wxy chooses scores of right labels
</span>  <span class="c1"># its shape is (#samples,)
</span>  <span class="n">wxy</span> <span class="o">=</span> <span class="p">[</span> <span class="n">wx</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">wx</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="p">]</span>
  
  <span class="c1"># judge expression
</span>  <span class="c1"># remember to exclude on y[i]'s
</span>  <span class="n">judge</span> <span class="o">=</span> <span class="n">wx</span> <span class="o">-</span> <span class="n">wxy</span> <span class="o">+</span> <span class="n">delta</span>
  <span class="c1"># make judge 0 on y[i]
</span>  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">wx</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="n">judge</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
  
  <span class="c1"># mass is a matrix holding all useful temp results
</span>  <span class="c1"># shape of judge is (#class, #train)
</span>  <span class="n">mass</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">judge</span><span class="p">)</span>
  
  <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">mass</span><span class="p">)</span> <span class="o">/</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">loss</span> <span class="o">+=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">W</span> <span class="o">*</span> <span class="n">W</span><span class="p">)</span>
</code></pre></div></div>

<p>Now explain on the <code class="language-plaintext highlighter-rouge">dW</code> part. In each iteration under <code class="language-plaintext highlighter-rouge">i</code>, <code class="language-plaintext highlighter-rouge">dW</code> adds an <code class="language-plaintext highlighter-rouge">ddW</code>. Each row of <code class="language-plaintext highlighter-rouge">ddW</code> is a weight timing <code class="language-plaintext highlighter-rouge">x[:, i]</code>. For most rows whoâ€™s not <code class="language-plaintext highlighter-rouge">y[i]</code>, weight is 1, i.e. <code class="language-plaintext highlighter-rouge">ddW[i] = x[:, i]</code>. For the specific row who is <code class="language-plaintext highlighter-rouge">y[i]</code>, <code class="language-plaintext highlighter-rouge">ddW[i]</code>â€™s weight is the negative sum of all other rows. This logic corresponds to the definition of \(\nabla_{w_{j}} L_{i}\):</p>

\[\nabla_{w_{j}} L_{i} = \cases {
{\bf 1}(w_{j}x^{(i)} - w_{y_{i}}x^{(i)} + \Delta &gt; 0) \cdot x^{(i)} &amp; \text{, if } j \ne y_{i} \\
-\left( \sum_{j \ne y_{i}}{\bf 1}(w_{j}x^{(i)} - w_{y_{i}}x^{(i)} + \Delta &gt; 0) \right) \cdot x^{(i)} &amp; \text{, otherwise}
}\]

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">svm_loss_vectorized</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span>
  <span class="c1"># continue on last snippet
</span>  <span class="c1"># weight to be producted by X
</span>  <span class="c1"># its shape is (#classes, #samples)
</span>  <span class="n">weight</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">((</span><span class="n">judge</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">))</span>
  
  <span class="c1"># weights on y[i] needs special care
</span>  <span class="n">weight_yi</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">wx</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="n">weight</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_yi</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
  
  <span class="c1"># half vectorized
</span>  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="n">ddW</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">weight</span><span class="p">[:,</span> <span class="n">i</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">dW</span> <span class="o">+=</span> <span class="n">ddW</span>
    
  <span class="n">dW</span> <span class="o">/=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">dW</span> <span class="o">+=</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">W</span>
  
  <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">dW</span>
</code></pre></div></div>

<h4 id="comparison-of-the-two-loop-version-and-the-half-vectorized-version">Comparison of the two-loop version and the half vectorized version</h4>

<p>Following the ipython notebook and testing on training dataset with shape (3073, 49000) and 10 labels, training of the two-loop version takes around <code class="language-plaintext highlighter-rouge">7s</code> and that of half vectorized version takes around <code class="language-plaintext highlighter-rouge">4s</code>. Actually the complexity of the algorithm doesnâ€™t change, i.e. $O \left( N \cdot D \cdot C \right)$; That speed goes up is because Python is slow in for loops, and maybe numpy has done some optimization in matrix multiplication.</p>

<h4 id="gradient-descent">Gradient descent</h4>

<p>This part is comparatively easy. Having gradient $\nabla_{W}L$, we may just subtract $W$ by $\alpha \nabla_{W}L$ in each iteration, where $\alpha$ is learning rate.</p>

<h3 id="predict-1">Predict</h3>

<p>The process of prediction is simply choosing the label with highest score. I omit it here.</p>

<h2 id="3-softmax">3. Softmax</h2>

<p>Keep the symbols defined in SVM, here we have:</p>

<ul>
  <li>
    <p>Score function: 
\(f(x^{(i)}, W) = W x^{(i)}\)</p>
  </li>
  <li>
    <p>Output function:
\(h(x^{(i)}, W) = \frac{e^{f_i}}{\sum_j e^{f_j}}\)</p>
  </li>
  <li>
    <p>Loss function: 
\(L_{i} = -f_{y_{i}} + \log \sum_{j}{e^{f_{j}}}\)</p>
  </li>
  <li>
    <p>Gradient function:
\(\eqalign{
\nabla_{W}L_{i} 
&amp;= -\nabla_{W}f^{(i)}_{y_{i}} + \frac{1}{\sum_{j}e^{f^{(i)}_{j}}} \sum_{j} \left( e^{f^{(i)}_{j}} \cdot \nabla_{W}f^{(i)}_{j} \right) \\
}\)</p>
  </li>
</ul>

<p>, where \(\nabla_{W_k}L_{f_{j}^{(i)}} = \cases{
x^{(i)} &amp; \text{, if } k = j \\
0 &amp; \text{, otherwise}
}\)</p>

<p>Or you may refer to <a href="http://deeplearning.stanford.edu/wiki/index.php/Softmax_Regression">this version</a> of gradient function.</p>

<ul>
  <li>
    <p>Total loss:
\(L = \frac{1}{N}\sum_{i}{L_{i}} + \frac{1}{2}\lambda\sum_{k,l}{w_{k,l}^{2}}\)</p>
  </li>
  <li>
    <p>Gradient of total loss:
\(\nabla_{W}L = \frac{1}{N}\sum_{i}{\nabla_{W} L_{i}} + \lambda W\)</p>
  </li>
</ul>

<p>According to <a href="http://cs231n.github.io/linear-classify/#softmax">the lecture note</a>, here we adjust \(f^{(i)}_{j}\) to \(f^{(i)}_{j} - \max_{j}{f^{(i)}_{j}}\). Hereâ€™s the snippet, very similar to that of SVM:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">softmax_loss_naive</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reg</span><span class="p">):</span>
  <span class="n">dW</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
  
  <span class="c1">### loss
</span>  <span class="n">scores</span> <span class="o">=</span> <span class="n">W</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
  <span class="n">scores_max</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">scores</span> <span class="o">-=</span> <span class="n">scores_max</span>
  <span class="n">exp_scores</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
  <span class="n">sums</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">exp_scores</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">log_sums</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">sums</span><span class="p">)</span>
  <span class="n">scores_y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">scores</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])])</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="o">-</span><span class="n">scores_y</span> <span class="o">+</span> <span class="n">log_sums</span><span class="p">)</span>

  <span class="n">loss</span> <span class="o">/=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">loss</span> <span class="o">+=</span> <span class="p">.</span><span class="mi">5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">W</span> <span class="o">*</span> <span class="n">W</span><span class="p">)</span>

  <span class="c1">### dW
</span>  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">xrange</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="c1"># dW += 1./sums[i] * log_sums[i] * X[:, i]
</span>    <span class="n">dW</span> <span class="o">+=</span> <span class="mf">1.</span><span class="o">/</span><span class="n">sums</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">exp_scores</span><span class="p">[:,</span> <span class="n">i</span><span class="p">].</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>
    <span class="n">dW</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">-=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span>

  <span class="n">dW</span> <span class="o">/=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
  <span class="n">dW</span> <span class="o">+=</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">W</span>
  
  <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">dW</span>
</code></pre></div></div>

<p>Note the sigma term
\(\sum_{j} \left( e^{f^{(i)}_{j}} \cdot \nabla_{W}f^{(i)}_{j} \right)\).
In Python it is interpreted as <code class="language-plaintext highlighter-rouge">exp_scores[:, i].reshape(-1, 1) * X[:, i]</code>, because for each <code class="language-plaintext highlighter-rouge">j</code>, thereâ€™s only one none zero row in $\nabla_{W}f^{(i)}_{j}$, whose value is $e^{f^{(i)}}$ dot product $x^{(i)}$. Shape of $e^{f^{(i)}}$ is <code class="language-plaintext highlighter-rouge">(C,)</code> and shape of $x^{(i)}$ is <code class="language-plaintext highlighter-rouge">(D,)</code>. Making use of broadcast, we can reshape $e^{f^{(i)}}$ to <code class="language-plaintext highlighter-rouge">(C, 1)</code> and make a <code class="language-plaintext highlighter-rouge">(C, D)</code> matrix; that is what we want for <code class="language-plaintext highlighter-rouge">ddW</code>.</p>

<h2 id="say-last">Say last</h2>
<p>Thank everyone that points out my mistakes. If thereâ€™s any more mistake, please donâ€™t hesitate to tell me. The original purpose of this blog is just to note down my immediate thoughts when following cs231n in my spare time. Itâ€™s totally written in an amateurâ€™s view; I would be happy if it helps you.</p>

<p>The official version of SVM and Softmax is really nice, and comes in later assignments.</p>
:ET