I"$<div class="posts">
  
    <article class="post">

      <h1><a href="/On-Dynamic-Mode-Decomposition/">On Dynamic Mode Decomposition</a></h1>
      
      <div class="date">
         Written on December 28, 2020
      </div>

      
        <div>
          <h3 id="background">Background</h3>
<p>I bought a book on data-driven dynamical system earlier this year right before the pandemic spread in the US (ref. 1). The book was writen by two Professors at University of Washington and was published quite recently. It talks about dynamical systems, but instead of focusing on the classical theories that we learnt in school, it focuses more on how data can help us identify and reduce the order of the systems. This post summarizes my understanding on the Dynamic Mode Decomposition (DMD) method after reading Session 7.2 of the book.</p>


        </div>
        <input type="checkbox" class="read-more-state" id="/On-Dynamic-Mode-Decomposition/" />
        <div class="read-more-1">
          
<h3 id="what-dmd-does">What DMD Does</h3>
<p>My understanding of DMD is that it tries to learn a linear dynamical system from data. Unlike training a deep learning neural network, however, DMD aims to do the ‚Äúlearning‚Äù fast and efficiently. The data fed into DMD are pairs of the state of the system separated by a small time interval $\Delta t$. By digesting these pairs of ‚Äúbefore and after‚Äù data, DMD learns to predict how the system evolves in $\Delta t$. And if you know how the system changes in a small time scale $\Delta t$, you can predict its state at any time after an initial state is given.</p>

<h3 id="how-dmd-works">How DMD Works</h3>
<p>Let‚Äôs first talk about the data fed into the DMD algorithm. Denote $\vec{x}=[x_1, x_2, \cdots, x_n]^T$ as the state of a $n$-degrees-of-freedom dynamical system. Note that it is a column vector. We select a few, say $m$, snapshots of the system at different time and arrange these column vectors into a matrix $\textbf{X} = [\vec{x}(t_1), \vec{x}(t_2), \cdots, \vec{x}(t_m)]$. At each snapshot, we also get the state of the system after a time delay $\Delta t$, and arrange these states in a second matrix: $\textbf{X}‚Äô = [\vec{x}(t_1+\Delta t), \vec{x}(t_2+\Delta t), \cdots, \vec{x}(t_m+\Delta t)]$. We could like to find a linear dynamical system such that:
\begin{equation}
\textbf{X}‚Äô = \textbf{A} \textbf{X}.
\end{equation}</p>

<p>Naturally, one would think that if we could do an ‚Äúinverse‚Äù of the $n \times m$ matrix $\textbf{X}$ so that $\textbf{A} = \textbf{X}‚Äô \textbf{X}^{-1}$, then mission is accomplished. This is indeed a valid thought and SVD (singular value decomposition) can help us perform the psudo inverse. The problem is that when we have a large $n$, computing the $n \times n$ matrix $\textbf{A}$ becomes expensive, not to mention that subsequently we may need to further compute some properties of the system by manipulating $\textbf{A}$. DMD gave us a procedure to find $\textbf{A}$ efficiently.
Since the main issue is that we may have large $n$, let‚Äôs imagine we have a magic solution to reduce the order of the system so that:
\begin{equation}
\vec{x} = \textbf{U} \vec{y}.
\end{equation}
Here $\textbf{U}$ is a $n \times r$ matrix, and $\vec{y}$, our magic new state with less degrees of freedom, is a $r \times 1$ vector with $r &lt; n$.</p>

<p>In the reduced space of $y$, the $r \times r$ linear transformation matrix $\textbf{A}_y$ is less expensive to compute:
\begin{equation}
\textbf{Y}‚Äô = \textbf{A}_y \textbf{Y}.
\end{equation}
And we can recover our real $\textbf{A}$ using $\textbf{U}^T \textbf{A}_y \textbf{U}$.</p>

<p>The final trick is decouple the linear system by yet another transform. This is the classical eigen problem exercise:
\begin{equation}
\vec{y} = \textbf{W} \vec{z},
\end{equation}
so that 
\begin{equation}
\textbf{Z}‚Äô = \textbf{A}_z \textbf{Z}.
\end{equation}
Here $\textbf{A}_z$ is a $r \times r$ diagonal matrix. In the $z$ space, the decoupled variables evolve following:
\begin{equation}
z_i(t) = \exp \left(\lambda_i t \right) z_i(0).
\end{equation}
Here $i=1,2,\cdots,r$.</p>

<p>To summarize, we need to do two transforms, one from $X$ to $Y$, followed by the second one from $Y$ to $Z$, to find the simple dynamics that matches the data. Then we revert those transform to recover how</p>

<h2 id="reference">Reference</h2>
<p>1.Brunton, S. L. &amp; Kutz, J. N. Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control. (Cambridge University Press, 2019). doi:10.1017/9781108380690.</p>

        </div>
        <label for="/On-Dynamic-Mode-Decomposition/" class="read-more-trigger"></label>
      

    </article>
  
    <article class="post">

      <h1><a href="/On-Transport-Maps/">On Transport Maps</a></h1>
      
      <div class="date">
         Written on December 25, 2020
      </div>

      
        <div>
          <h3 id="background">Background</h3>
<p>I‚Äôve been working with Professor Y. Marzouk and his postdoc P. Rubio on a Schlumberger project for the past year. One interesting technique I learnt from them is the so-called transport map. 
It is not about how to fit all MBTA routes onto an A4 paper, although everytime I saw the MBTA map on a bus I wonder if there is any math behind the design of that map. The transport map I learnt during the past year is about how to represent a complex probability distribution. Without sharing any Schlumberger related things, this post tries to document my understanding of the basics of transport maps (all have been published by Marzouk earlier, see ref.1 for example).</p>


        </div>
        <input type="checkbox" class="read-more-state" id="/On-Transport-Maps/" />
        <div class="read-more-1">
          
<h3 id="what-transport-maps-can-do-for-us">What Transport Maps Can Do for Us</h3>
<p>Imagine the following scenario. We know $x$ is a random variable that follows a standard Gaussian distribution. There is a complex function (maybe the function is only available numerically) $y=f(x)$ that transforms any given $x$ to a scalar $y$. What can we say about the distribution $P(y)$? Apparently one can sample a lot of $x$, pass them to the function $f$ to generate samples of $y$, then use statistics to describe the $y$ samples. This is great. But sometimes we may want to get an analytical expression for $P(y)$, or in some cases we are only given some samples of $y$ without any knowledge of $x$ and $f$ but we want to have a machinary to keep generating new samples of $y$ that minic the given samples. Transport maps can help.</p>

<p>Another scenario transport maps can help is when we are given an un-normalized distribution (maybe only numerically, i.e., if you give me a value $x$, I can tell you $P(x) \sim h(x)$ where $h$ doesn‚Äôt have an analytical expression but can be computed numerically following some procedure). Transport map can help us construct a normalized distribution and subsequently draw samples from it.</p>

<h3 id="how-transport-maps-work">How Transport Maps Work</h3>
<p>A transport map is a transform between a reference distribution and a targe distribution. We can use any simple distribution we like as our reference distribution and the target distribution is the one that we want to describe. The map is parametrized. The goal is to optimize those parameters so that after the transformation, the obtained target distribution matches any information we have about the target distribution. At the end of the day, finding a transport map becomes a loss minimization problem for parameter optimization.</p>

<p>Let‚Äôs say we are given some samples of $y$ and would like to (1) Obtain an analytic expression for $P(y)$ and (2) Generate more samples of $y$ ourselves. First thing we do is to pick a reference distribution we like. Say a standard normal distribution $P_x(x) \sim N(0,1)$. Then we define a parametrized map $y=f_\theta(x)$. Here $\theta$ are the parameters we will need to find/optimize later. Given any set of the parameters $\theta$, we can express $P_y(y)$ analytically as $P_y(y) = P_x({f_\theta}^{-1}(y)) | \nabla {f_\theta}^{-1} |$. Basically given any $y$, we say let‚Äôs transform it back to $x$ via $f_\theta^{-1}$ and see what‚Äôs the probability that $x$ appears ($P_x(x)$). We then adjust that with the Jacobian of the transform $f{_\theta}^{-1}$. With an parametrized and analytic $P_y(y)$, we now optimize the parameters so that $P_y(y)$ matches with the samples of $y$ given to us. This optimization can be done by minimizing the KL divergence between $P_y$ and the samples.</p>

<h3 id="references">References</h3>
<p>1.Marzouk, Y., Moselhy, T., Parno, M. &amp; Spantini, A. An introduction to sampling via measure transport. arXiv:1602.05023 [math, stat] 1‚Äì41 (2016) doi:10.1007/978-3-319-11259-6_23-1.</p>

        </div>
        <label for="/On-Transport-Maps/" class="read-more-trigger"></label>
      

    </article>
  
    <article class="post">

      <h1><a href="/Helllo-World/">Hello World</a></h1>
      
      <div class="date">
         Written on December 24, 2020
      </div>

      
        <p>Thanks for visiting this site. I created it using Jekyll.</p>

      

    </article>
  
</div>
:ET